{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys, scipy, numpy\n",
    "import json\n",
    "from spacy.lang.ru import Russian\n",
    "from spacy_russian_tokenizer import RussianTokenizer, MERGE_PATTERNS\n",
    "import en_core_web_sm\n",
    "import pymorphy2\n",
    "import textacy.extract\n",
    "import stanza\n",
    "from spacy_stanza import StanzaLanguage\n",
    "import stanfordnlp\n",
    "import scipy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import operator\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression,  LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the figure title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "import pkg_resources\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob, Word\n",
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import datetime\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popular stop words that should also \n",
    "pop_words = ['go', 'would', 'get', 'new', 'time', 'take', 'can', 'to',\n",
    "             'come', 'let', 'really', 'de', 'okay', 'people', 'thing', 'also', \n",
    "            'thanks', 'well', 'say', 'see', 'year', 'yeah', 'lol', 'point', 'make', 'oh', 'god', 'thank', \n",
    "            'follow', 'send', 'dm', 'www', 'many', 'much', 'day', 'wow', 'know', 'mao',\n",
    "             'instagram', 'link', 'eye', 'want', 'look', 'oh', 'hi', 'sure', 'right', 'lot', 'think', \n",
    "             'fuck', 'video', 'cop', 'even', 'every', 'post', 'shit', 'come', 'always', 'still',\n",
    "            'call', 'haircut', 'feel','bro', 'of', 'take', 'though', 'awww', 'he', 'she',\n",
    "            'watch', 'sub', 'ban', 'much', 'share', 'thank', 'show', 'page', 'someone',\n",
    "            'could', 'use', 'give', 'ask', 'ever', 'way', 'could', 'omg', 'give', 'something', 'time',\n",
    "            'maybe', 'damn', 'since', 'haha', 'shower', 'name', 'mean', 'might', 'everything', 'others', \n",
    "             'comment', 'fox', 'clearly', 'talk', 'fact', 'ilm', 'nfc', 'try', 'never', 'hey', 'must', \n",
    "             'big', 'hair', 'without', 'miss', 'else', 'ok', 'not', 'two', 'do', 'anyone', 'put', 'bring',\n",
    "            'first', 'anything', 'thanks', 'fav', 'may', 'need', 'exactly', 'one', 'amp',\n",
    "            'http', 'co', 'medium','https', 'actually', 'nothing', 'back', 'word', 'com', 'imgur', \n",
    "            'either', 'congrats', 'op', 'though', 'draft', 'sooo', 'xx', 'riotgrrrl',\n",
    "            'riotgrrrl', 'batmobile', 'riotgrrrlzine', 'riotgrrrlroots', 'riot', 'grrrl', 'heavenstobetsy',\n",
    "            'photo', 'eat', 'morning', 'babe', 'wing', 'karen', 'bee', 'tag', 'hat', 'dc', 'na',\n",
    "            'boyslockerroomtruth', 'arrestswatimaliwal', 'tweet', 'ah', 'seriously', 'sunday', 'grow',\n",
    "            'whaaaaaaat', 'whoaaaaa', 'shhhhhh', 'canvass', 'moore', 'aw', 'tho', 'yeahhhhhh', 'wish', \n",
    "             'alway', 'awhile', 'case', 'play', 'add', 'ma','da', 'note', 'em', 'term',\n",
    "            'find', 'less', 'yup', 'tds', 'caption', 'read', 'ro', 'apt', 'stuff', 'almost', 'sort', '10000a',\n",
    "            'phone', 'nowwhats', 'nah', 'alexa', 'so', 'ummmm', 'next', 'waaaaaaaaaaa', 'nearly', 'as', \n",
    "             'probably', 'sooooo', 'book', 'entire', 'tb', 'fall', 'metro', 'bro','don', 'today',\n",
    "             'update', 'tell', 'bathroom',\n",
    "            'ed', 'yet', 'three', 'huh', 'left', 'onion', 'seem', 'meow', 'shapiro', 'everyone', 'guess',\n",
    "             'real', 'load']\n",
    "# marks of spanish words to be removed\n",
    "spanish = ['la', 'qe', 'que', 'te', 'linda', 'ki', 'ya', 'que', 'olhar', 'puro', 'teu', 'md',\n",
    "          'esse', 'olhar', 'essa', 'boca', 'esses', 'olhos', 'esse', 'corp', 'uma', 'cobra', 'prima',\n",
    "          'eu', 'tbm', 'tomei', 'liberdade', 'de', 'postal', 'meu', 'se',\n",
    "           'essa', 'de', 'segunda', 'feira', 'tbm', 'doux', 'merci', 'soil', 'douce', 'bon',\n",
    "           'end', 'week', 'belle', 'rie', 'ch','stu',  'tu', 'deus', 'aren', 'oe', 'voc',\n",
    "          'mu', 'yle', 'lanka', 'si', 'eds', 'kemal', 'kondo', 'meshrefet', 'yemymrebsh',\n",
    "           'aten', 'oluyorlar', 'bir', 'sm', 'ger', 'eli', 'olan', 'drum', 'asia', 'genel', 'ildir',\n",
    "           'kad', 'ine', 'despite', 'olmuyor', 'erkek', 'al', 'sada', 'al', 'masada', 'line', 'medi', 'bir', 'ge', \n",
    "        'ev', 'kad', 'revi', 'ilse', 'al', 'ev', 'ge', 'indirmekte', 'erie', 'revi', 'ildir', 'ama', \n",
    "           'kad', 'eve', 'kal', 'hem', 'erkek', 'al', 'hem', 'erkek', 'evi', 'ge', 'incision', 'diyip', 'sorumluluklar', \n",
    "           'bekliyor', 'sorumluluk', 'palmyra', 'kad', 'near', 'sorumluluklarda', 'beklemesinler', 'al', 'erie', 'al', \n",
    "           'mayan', 'kar', 'ev', 'ramadi', 'riek', 'gayet', 'normal', 'sonu', 'ta', 'al', 'ine', 'yard', 'between', 'kad',\n",
    "           'ev', 'index', 'bana', 'yard', 'etmiyor', 'die', 'panama', 'erkek', 'al', 'ev', 'ge', 'indiriyorsa', 'kad', 'nda', \n",
    "           'eve', 'kal', 'yorsa', 'kad', 'ev', 'yapar', 'sorumluluk', 'editor', 'aman', 'al', 'kocas', 'yard', 'etsin'\n",
    "           'gracie', 'slesha',  'totalmente', 'um', 'mundo', 'aparte', 'soho', 'de', 'consumo',\n",
    "           'vo', 'te', 'quite', 'mi', 'amor', 'todo', 'santo', 'dia', 'um', 'gaston', 'de', 'energia', 'fa', 'passar', 'raiva', 'forabolsonaro']\n",
    "# mapping similar words to unite\n",
    "sense_mapping = {'feminist': 'feminism', 'men': 'man', 'women': 'woman', 'girl': 'woman', 'boy': 'man', 'male': 'man', 'female': 'woman',\n",
    "                'guy': 'man', 'babe': 'woman', 'godbless': 'bless', 'harass': 'harassment', 'soo': 'so', 'hoping': 'hope', 'ppl': 'people'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_similar_words(text):\n",
    "    text = ' '.join(text)\n",
    "    for k, v in sense_mapping.items():\n",
    "        text = text.replace(' %s ' % k, ' %s ' % v)\n",
    "    return text.split()\n",
    "\n",
    "def filter_pop_words(text):\n",
    "    return [word for word in text if not word in pop_words]\n",
    "\n",
    "def remove_spanish(text):\n",
    "    for s in spanish:\n",
    "        if s in text:\n",
    "            return []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(tokens):\n",
    "    tokens = list(map(lambda t: map_similar_words(t), tokens))\n",
    "    tokens = list(map(lambda t: filter_pop_words(t), tokens))\n",
    "    tokens = list(map(lambda t: remove_spanish(t), tokens))\n",
    "    tokens = list(filter(lambda t: len(np.unique(t)) > 1 and len(t) > 1, tokens))\n",
    "    return tokens\n",
    "\n",
    "def process_tokens_df(df_tokens, param):\n",
    "    df_tokens[param] = df_tokens[param].apply(lambda t: map_similar_words(t.split()))\n",
    "    df_tokens[param] = df_tokens[param].apply(lambda t: filter_pop_words(t))\n",
    "    df_tokens[param] = df_tokens[param].apply(lambda t: remove_spanish(t))\n",
    "    df_tokens = df_tokens[df_tokens[param].map(lambda t: (len(np.unique(t)) > 1) and (len(t) > 1))]\n",
    "    return df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts, lsi=False):\n",
    "    for idx, topic in ldamodel.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "    cols = ['dominant_topic', 'perc_contrib', 'keywords', 'probs', ]\n",
    "    sent_topics_df = pd.DataFrame(index=range(len(corpus)), columns=cols)\n",
    "\n",
    "    for i, row in tqdm(list(enumerate(ldamodel[corpus]))):\n",
    "        probs = [x[1] for x in row]\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        if len(row) > 0:\n",
    "            topic_num, prop_topic = row[0]\n",
    "            wp = ldamodel.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            sent_topics_df.iloc[i,:] = [int(topic_num), round(prop_topic,4), topic_keywords, probs]\n",
    "            \n",
    "    sent_topics_df['dominant_topic'] = sent_topics_df['dominant_topic'].astype(float)\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts, name='text')\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "def best_sentences_for_topic(df_info, topic, n=5):\n",
    "    df = df_info[df_info['dominant_topic'] == topic].sort_values(by='perc_contrib', ascending=False)\n",
    "    return df.iloc[:n,-1].values\n",
    "\n",
    "def get_topic_desc(ldamodel, df_info, is_hdp, num_topics, num_words=5, n_sent=5):\n",
    "    if is_hdp:\n",
    "        hdp_topics = ldamodel.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "        for topic in hdp_topics:\n",
    "            print(topic)\n",
    "        print('-------')\n",
    "    else:\n",
    "        n_topics = ldamodel.num_topics\n",
    "        s = sorted(range(n_topics), reverse=True, key=lambda k: len(df_info[df_info['dominant_topic'] == k]))\n",
    "        lda_topics = ldamodel.print_topics()\n",
    "        for k in s:\n",
    "            cluster_size = len(df_info[df_info['dominant_topic'] == k])\n",
    "            if cluster_size > 0:\n",
    "                print('Cluster:', k, 'size -', cluster_size)\n",
    "                keywords = [p[0] for p in ldamodel.show_topic(k)]\n",
    "                sentences = best_sentences_for_topic(df_info, k, n_sent)\n",
    "                print('Keywords:', keywords)\n",
    "                print('Keywords:', lda_topics[k][1])\n",
    "                print(sentences)\n",
    "                print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tm_model(tokens, model, num_topics, filter_pop, mins, maxs):\n",
    "    id2word = corpora.Dictionary(tokens)\n",
    "    print('dictionary size:', len(id2word))\n",
    "\n",
    "    id2word.filter_extremes(no_below=mins, no_above=maxs)\n",
    "    print('short dictionary size:', len(id2word))\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in tokens]\n",
    "\n",
    "    if model == 'lda':\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           alpha=0.1,\n",
    "                                           eta=0.01,\n",
    "                                           random_state=666,\n",
    "                                           chunksize=len(tokens),\n",
    "                                           passes=10)\n",
    "        cm = gensim.models.CoherenceModel(\n",
    "            model=lda_model, texts=tokens, dictionary=id2word, coherence='c_v', processes=1)\n",
    "        return lda_model, corpus, id2word, cm.get_coherence()\n",
    "\n",
    "    if model == 'lsi':\n",
    "        lsi_model = gensim.models.LsiModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           chunksize=len(tokens),\n",
    "                                           num_topics=num_topics)\n",
    "        cm = gensim.models.CoherenceModel(\n",
    "            model=lsi_model, texts=tokens, dictionary=id2word, coherence='c_v', processes=1)\n",
    "        return lsi_model, corpus, id2word, cm.get_coherence()\n",
    "\n",
    "    if model == 'hdp':\n",
    "        hdp_model = gensim.models.HdpModel(corpus=corpus, id2word=id2word, chunksize=len(tokens))\n",
    "        cm = gensim.models.CoherenceModel(\n",
    "            model=hdp_model, texts=tokens, dictionary=id2word, coherence='c_v', processes=1)\n",
    "        return hdp_model, corpus, id2word, cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tm_model_simple(texts, model, min_corp, max_corp, num_topics):\n",
    "    texts_proc = list(map(lambda d: d.split(), texts.unique())) \n",
    "    tokens = process_tokens(texts_proc)\n",
    "    id2word = corpora.Dictionary(tokens)\n",
    "    print('dictionary size:', len(id2word))\n",
    "\n",
    "    id2word.filter_extremes(no_below=min_corp, no_above=max_corp)\n",
    "    print('short dictionary size:', len(id2word))\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in tokens]\n",
    "\n",
    "    if model == 'lda':\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           chunksize=len(texts_proc))\n",
    "        return lda_model, corpus, id2word, texts_proc\n",
    "\n",
    "    if model == 'lsi':\n",
    "        lsi_model = gensim.models.LsiModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           chunksize=len(texts_proc))\n",
    "        return lsi_model, corpus, id2word, texts_proc\n",
    "\n",
    "    if model == 'hdp':\n",
    "        hdp_model = gensim.models.HdpModel(corpus=corpus, id2word=id2word, chunksize=len(texts_proc))\n",
    "        return hdp_model, corpus, id2word, texts_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_topics(text, model, mins, maxs, num_of_topic, num_words, num_sent, filter_pop=False, show=False):\n",
    "    text_pr = list(map(lambda d: d.split(), text.unique())) \n",
    "    tokens = process_tokens(text_pr)\n",
    "    tm_model, corpus, _dict, cm = get_tm_model(tokens, model, num_of_topic, filter_pop, mins, maxs)  \n",
    "    \n",
    "    df_info = format_topics_sentences(tm_model, corpus, text_pr, model != 'lda')\n",
    "    get_topic_desc(tm_model, df_info, model == 'hdp', num_of_topic, num_words, num_sent)\n",
    "    \n",
    "    if show:\n",
    "        return tm_model, corpus, _dict\n",
    "    return cm, df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_topics_df(df_text, model, mins, maxs, num_of_topic, num_words, num_sent, filter_pop=False, show=False):\n",
    "    df_text = df_text.drop_duplicates(subset=['text_proc_tm'])\n",
    "    df_text['text_proc_tokens'] = df_text['text_proc_tm']\n",
    "    df_text_tokens = process_tokens_df(df_text, 'text_proc_tokens')\n",
    "    tm_model, corpus, _dict, cm = get_tm_model(df_text_tokens['text_proc_tokens'], model, \n",
    "                                               num_of_topic, filter_pop, mins, maxs)  \n",
    "    txt_proc = list(map(lambda d: d.split(), list(df_text_tokens['text_proc_tm'].values))) \n",
    "    \n",
    "    df_info = format_topics_sentences(tm_model, corpus, txt_proc, model != 'lda')\n",
    "    get_topic_desc(tm_model, df_info, model == 'hdp', num_of_topic, num_words, num_sent)\n",
    "    \n",
    "    if show:\n",
    "        return tm_model, corpus, _dict\n",
    "    return cm, df_info, df_text_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        feature_words = np.unique([feature_names[i]\n",
    "                                   for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print(\" \".join(feature_words))\n",
    "\n",
    "\n",
    "def process_nmf(text, num_topics, num_words):\n",
    "    tokens = list(map(lambda d: d.split(), text.unique()))\n",
    "    tokens = process_tokens(tokens)\n",
    "    doc = list(map(lambda t: ' '.join(t), tokens))\n",
    "    num_topics = min(num_topics, len(doc))\n",
    "    no_features = 1500\n",
    "    # NMF is able to use tf-idf\n",
    "    try:\n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_df=0.8, min_df=10, max_features=no_features, stop_words='english')\n",
    "        tfidf = tfidf_vectorizer.fit_transform(doc)\n",
    "        tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "        # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "        tf_vectorizer = CountVectorizer(\n",
    "            max_df=0.8, min_df=10, max_features=no_features, stop_words='english')\n",
    "        tf = tf_vectorizer.fit_transform(doc)\n",
    "        tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "        # Run NMF\n",
    "        nmf = NMF(n_components=num_topics, random_state=666,\n",
    "                  alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "        # Run LDA\n",
    "        lda = LatentDirichletAllocation(n_components=num_topics, max_iter=4,\n",
    "                                        learning_method='online', learning_offset=80., random_state=666).fit(tf)\n",
    "\n",
    "        print('NMF')\n",
    "        display_topics(nmf, tfidf_feature_names, num_words)\n",
    "        print('LDA')\n",
    "        display_topics(lda, tf_feature_names, num_words)\n",
    "    except Exception:\n",
    "        print('exception occured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_graph(texts, limit):\n",
    "    texts_parsed = list(map(lambda d: d.split(), texts.unique()))\n",
    "    tokens = process_tokens(texts_parsed)\n",
    "    id2word = corpora.Dictionary(tokens)\n",
    "    print('dictionary size:', len(id2word))\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.75)\n",
    "    print('short dictionary size:', len(id2word))\n",
    "    corpus = [id2word.doc2bow(text) for text in tokens]\n",
    "\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        print(num_topics)\n",
    "        lsi_model = gensim.models.LsiModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics)\n",
    "        lm_list.append(lsi_model)\n",
    "        cm = gensim.models.CoherenceModel(model=lsi_model, texts=texts_parsed, dictionary=id2word,\n",
    "                                          coherence='c_v', processes=1, topn=10)\n",
    "        c_v.append(cm.get_coherence())\n",
    "\n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_graph_tokens(texts, limit):\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    all_info = []\n",
    "    for below in range(5, 30, 5):\n",
    "        sub_c_v = []\n",
    "        for above in range(3, 10, 1):\n",
    "            print(f'below {below}, above {0.1 * above}')\n",
    "            texts_parsed = list(map(lambda d: d.split(), texts.unique()))\n",
    "            tokens = process_tokens(texts_parsed)\n",
    "            id2word = corpora.Dictionary(tokens)\n",
    "            print('dictionary size:', len(id2word))\n",
    "            id2word.filter_extremes(no_below=below, no_above=0.1*above)\n",
    "            print('short dictionary size:', len(id2word))\n",
    "            corpus = [id2word.doc2bow(text) for text in tokens]\n",
    "\n",
    "            for num_topics in range(1, limit):\n",
    "                print(num_topics)\n",
    "                lsi_model = gensim.models.LsiModel(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=num_topics)\n",
    "                lm_list.append(lsi_model)\n",
    "                cm = gensim.models.CoherenceModel(model=lsi_model, texts=texts_parsed, dictionary=id2word,\n",
    "                                                  coherence='c_v', processes=1, topn=10)\n",
    "                q = cm.get_coherence()\n",
    "                c_v.append(q)\n",
    "                all_info.append([below, 0.1*above, num_topics, q])\n",
    "                print(below, 0.1*above, num_topics, q)\n",
    "                sub_c_v.append(q)\n",
    "\n",
    "        x = list(range(1, len(sub_c_v) + 1))\n",
    "        dta = list(map(list, zip(x, sub_c_v)))\n",
    "        print(dta)\n",
    "        df = pd.DataFrame(dta, columns=[\"x\", \"y\"])\n",
    "        ax = sns.lineplot(x=\"x\", y=\"y\", data=df)\n",
    "        plt.show()\n",
    "\n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_top_model(texts, model):\n",
    "    texts_parsed = list(map(lambda d: d.split(), texts.unique()))\n",
    "    tokens = process_tokens(texts_parsed)\n",
    "    id2word = corpora.Dictionary(tokens)\n",
    "    print('dictionary size:', len(id2word))\n",
    "    id2word.filter_extremes(no_below=20, no_above=0.6)\n",
    "    print('short dictionary size:', len(id2word))\n",
    "    corpus = [id2word.doc2bow(text) for text in tokens]\n",
    "\n",
    "    top_topics = [(0, 0)]\n",
    "    while top_topics[0][1] < 0.5:\n",
    "        lm = None\n",
    "        if model == 'lda':\n",
    "            lm = gensim.models.LdaModel(corpus=corpus, id2word=id2word)\n",
    "        if model == 'lsi':\n",
    "            lm = gensim.models.LsiModel(corpus=corpus, id2word=id2word)\n",
    "        if model == 'hdp':\n",
    "            lm = gensim.models.HdpModel(corpus=corpus, id2word=id2word)\n",
    "        coherence_values = {}\n",
    "        for n, topic in lm.show_topics(num_topics=-1, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            cm = gensim.models.CoherenceModel(\n",
    "                topics=[topic], texts=texts_parsed, dictionary=id2word, window_size=10, processes=1)\n",
    "            coherence_values[n] = cm.get_coherence()\n",
    "        top_topics = sorted(coherence_values.items(),\n",
    "                            key=operator.itemgetter(1), reverse=True)\n",
    "        print(top_topics[0][1])\n",
    "    return lm, top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, indices):\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.figure()\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_for_topic(model, x, y):\n",
    "    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "    cloud = WordCloud(stopwords=pop_words,\n",
    "                      background_color='white')\n",
    "\n",
    "    topics = model.show_topics(formatted=False)\n",
    "\n",
    "    fig, axes = plt.subplots(x, y, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        fig.add_subplot(ax)\n",
    "        topic_words = dict(topics[i][1])\n",
    "        topic_words = dict((k, abs(v)) for k, v in topic_words.items()) \n",
    "        print(topic_words)\n",
    "        cloud.generate_from_frequencies(topic_words)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data with Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_instagram = pd.read_csv('df_instagram', keep_default_na=False)\n",
    "df_twitter = pd.read_csv('df_twitter', keep_default_na=False)\n",
    "df_reddit = pd.read_csv('df_reddit', keep_default_na=False).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram['date'] = df_instagram['ts'].progress_apply(lambda t: pd.to_datetime(t, unit='s'))\n",
    "df_twitter['date'] = df_twitter['created_at'].progress_apply(lambda t: datetime.datetime.strptime(t, '%a %b %d %H:%M:%S %z %Y') if len(t) > 0 else None)\n",
    "df_reddit['date'] = df_reddit['utc'].progress_apply(lambda t: pd.to_datetime(t, unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_unique['text_proc_sentiment_prob'] = \\\n",
    "    df_twitter_unique['text_proc_sentiment_prob_pos'].\\\n",
    "       progress_apply(lambda t: 4 if t >= 0.675 else 0 if t <= 0.33 else 2)\n",
    "\n",
    "df_instagram_unique['text_proc_sentiment_prob'] = \\\n",
    "    df_instagram_unique['text_proc_sentiment_prob_pos'].\\\n",
    "       progress_apply(lambda t: 4 if t >= 0.675 else 0 if t <= 0.33 else 2)\n",
    "\n",
    "df_reddit_unique['text_proc_sentiment_prob'] = \\\n",
    "    df_reddit_unique['text_proc_sentiment_prob_pos'].\\\n",
    "        progress_apply(lambda t: (4 if float(t) >= 0.675 else 0 if float(t) <= 0.33 else 2) if len(str(t)) > 0 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram['text_length'] = df_instagram['text'].apply(lambda t: len(t))\n",
    "df_twitter['text_length'] = df_twitter['text'].apply(lambda t: len(t))\n",
    "df_reddit['text_length'] = df_reddit['text'].apply(lambda t: len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"instagram {df_instagram['text_length'].mean()}, twitter {df_twitter['text_length'].mean()}, reddit {df_reddit['text_length'].mean()}\")\n",
    "print(f\"instagram {df_instagram['text_length'].min()} - {df_instagram['text_length'].max()}, twitter {df_twitter['text_length'].min()} - {df_twitter['text_length'].max()}, reddit {df_reddit['text_length'].min()} - {df_reddit['text_length'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['lda', 'lsi', 'hdp']\n",
    "def group_by_data(df, param, num_topics, num_words=10, num_sent=5):\n",
    "    params_set = list(filter(lambda t: len(str(t)) > 0, np.unique(df[param])))\n",
    "    all_info = []\n",
    "    for p in params_set:\n",
    "        sub_df = df.loc[df[param] == p]\n",
    "        print(f'{param} {p}, records size = {len(sub_df)}')\n",
    "        if len(sub_df) > 200:\n",
    "            for m in methods:\n",
    "                print(f'method {m}')\n",
    "                cms = []\n",
    "                for s in range(1, num_topics):\n",
    "                    print(f'number of topics {s}')\n",
    "                    cm = construct_topics(sub_df['text_proc_tm'], m, s, num_words, num_sent, True)\n",
    "                    print(cm)\n",
    "                    process_nmf(sub_df['text_proc_tm'], s, num_words)\n",
    "                    cms.append(cm)\n",
    "                    all_info.append([p, m, s, cm])\n",
    "                    print()\n",
    "                    print()\n",
    "                print(cms)\n",
    "                print(f'Finished  with {param} and method {m}')\n",
    "                plt.figure()\n",
    "                x = range(1, num_topics)\n",
    "                plt.plot(x, cms)\n",
    "                plt.xlabel(\"num_topics\")\n",
    "                plt.ylabel(\"Coherence score\")\n",
    "                plt.legend((\"c_v\"), loc='best')\n",
    "                plt.show()\n",
    "    return cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['lda', 'lsi', 'hdp']\n",
    "\n",
    "def tm_by_methods(df, num_topics, num_words=10, num_sent=5):\n",
    "    for m in methods:\n",
    "        print(f'method {m}')\n",
    "        cms = []\n",
    "        for s in range(1, num_topics):\n",
    "            print(f'number of topics {s}')\n",
    "            cm = construct_topics(\n",
    "                df['text_proc_tm'], m, s, num_words, num_sent, True)\n",
    "            print(cm)\n",
    "            process_nmf(sub_df['text_proc_tm'], s, num_words)\n",
    "            cms.append(cm)\n",
    "            print()\n",
    "            print()\n",
    "        print(cms)\n",
    "        print(f'Finished  with method {m}')\n",
    "        plt.figure()\n",
    "        x = range(1, num_topics)\n",
    "        plt.plot(x, cms)\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel(\"num_topics\")\n",
    "        plt.ylabel(\"Coherence score\")\n",
    "        plt.show()\n",
    "    return cms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_unique = df_instagram.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = len(df_instagram_unique[df_instagram_unique[\"is_comment\"] == True][\"id\"])\n",
    "publ = len(df_instagram_unique[df_instagram_unique[\"is_comment\"] == False][\"id\"])\n",
    "publ_with_com = len(df_instagram_unique[(df_instagram_unique[\"is_comment\"] == False) & (df_instagram_unique['comments_count'] > 5)][\"id\"])\n",
    "print(f'overall length: {len(df_instagram_unique[\"id\"])}, comments: {com}, publications: {publ}, publications with comments: {publ_with_com}')\n",
    "print(f'avg comments per publication: {com/publ}, avg for non zero: {com/publ_with_com}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_instagram_unique.location_country.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_grouped_location = df_instagram_unique[(df_instagram_unique['location_country'] != '') & (df_instagram_unique['is_comment'] == False)].groupby(['location_country']).count().reset_index()\n",
    "print(len(inst_grouped_location[inst_grouped_location['text'] > 20]))\n",
    "# plt.figure(figsize=(15,20))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.barplot(y=\"location_country\", x=\"text\", data=inst_grouped_location[inst_grouped_location['text'] > 20])\n",
    "ax.set(ylabel='country code', xlabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_grouped_location = df_instagram_unique[(df_instagram_unique['location_country'] != '') \\\n",
    "                                            & (df_instagram_unique['is_comment'] == False)].\\\n",
    "                                              groupby(['location_country', 'text_proc_sentiment_prob']).count().reset_index()\n",
    "print(len(inst_grouped_location[inst_grouped_location['text'] > 20]))\n",
    "# plt.figure(figsize=(15,20))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.barplot(y=\"location_country\", x=\"text\", hue=\"text_proc_sentiment_prob\",\n",
    "                 data=inst_grouped_location[inst_grouped_location['text'] > 20])\n",
    "ax.set(ylabel='country code', xlabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_unique[df_instagram_unique['location_country'] != '']['location_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.distplot(df_instagram_unique[df_instagram_unique[\"is_comment\"] == False][\"likes\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax.set(xlabel='number of likes', ylabel='count of posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_instagram_unique['text_len'] = df_instagram_unique['text'].apply(lambda t: len(t))\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.distplot(df_instagram_unique[df_instagram_unique[\"is_comment\"] == False][\"text_len\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax.set(xlabel='number of likes', ylabel='count of posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_unique['text_len'] = df_instagram_unique['text'].apply(lambda t: len(t))\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.distplot(df_instagram_unique[\"text_len\"], \n",
    "             kde=False, rug=True, bins=40, label=\"sdfs\")\n",
    "ax.set(xlabel='text length in symbols', ylabel='count of posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.distplot(df_instagram_unique[(df_instagram_unique[\"is_comment\"] == False) & (df_instagram_unique[\"likes\"]<10000)][\"likes\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax.set(xlabel='number of likes', ylabel='count of posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "sns.distplot(df_instagram_unique[df_instagram_unique[\"is_comment\"] == False][\"comments_count\"], \n",
    "             kde=False, rug=True, bins=10, label=\"sdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "sns.distplot(df_instagram_unique[(df_instagram_unique[\"is_comment\"] == False) & (df_instagram_unique[\"comments_count\"] < 100)][\"comments_count\"], \n",
    "             kde=False, rug=True, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_unique['date_info'] = df_instagram_unique['date'].dt.month.map(str) + \"-\" + df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique['year'] = df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique_grouped = df_instagram_unique.groupby(['year']).count().reset_index()\n",
    "df_instagram_unique_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_unique['date_info'] = df_instagram_unique['date'].dt.month.map(str) + \"-\" + df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique['year'] = df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique_grouped = df_instagram_unique.groupby(['year']).count().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.barplot(x=\"year\", y=\"text\", data=df_instagram_unique_grouped)\n",
    "ax.set(xlabel='year of publications', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "\n",
    "df_instagram_unique['date_info'] = df_instagram_unique['date'].dt.month.map(str) + \"-\" + df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique['year'] = df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique_grouped = df_instagram_unique.groupby(['year', 'text_proc_sentiment_prob']).count().reset_index()\n",
    "\n",
    "\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax1 = sns.barplot(x=\"year\", y=\"text\", hue=\"text_proc_sentiment_prob\", data=df_instagram_unique_grouped, ax=axes[0])\n",
    "ax1.set(xlabel='year of publications', ylabel='count of posts and comments')\n",
    "\n",
    "df_instagram_unique['date_info'] = df_instagram_unique['date'].dt.month.map(str) + \"-\" + \\\n",
    "                                                                df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique['year'] = df_instagram_unique['date'].dt.year.map(str)\n",
    "df_instagram_unique_grouped = df_instagram_unique.groupby(['year', 'text_proc_sentiment']).count().reset_index()\n",
    "\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.barplot(x=\"year\", y=\"text\", hue=\"text_proc_sentiment\", data=df_instagram_unique_grouped, ax=axes[1])\n",
    "ax.set(xlabel='year of publications', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs = np.unique(df_instagram_unique_grouped['year'].values)\n",
    "prs = []\n",
    "for y in yrs:\n",
    "    subdf = df_instagram_unique_grouped[df_instagram_unique_grouped['year'] == y].groupby(['text_proc_sentiment']).sum()[['text']]\n",
    "    prs.append(subdf['text'][0]/(subdf['text'][0] + subdf['text'][4])*100)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('percent of negative texts')\n",
    "plt.plot(yrs,prs)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "sns.distplot(df_instagram_unique[(df_instagram_unique[\"is_comment\"] == False) & (df_instagram_unique[\"comments_count\"] < 10)][\"comments_count\"], \n",
    "             kde=False, rug=True, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst_tag = df_instagram[(df_instagram[\"is_comment\"] == False)].groupby(['tag']).count().reset_index()\n",
    "plt.figure(figsize=(10,15))\n",
    "ax = sns.barplot(y=\"tag\", x=\"text\", data=df_inst_tag)\n",
    "ax.set(xlabel='account or tag', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram['text_proc_sentiment_prob'] = \\\n",
    "    df_instagram['text_proc_sentiment_prob_pos'].\\\n",
    "       progress_apply(lambda t: 4 if t >= 0.675 else 0 if t <= 0.33 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REC = 200\n",
    "f, axes = plt.subplots(2, 2, figsize=(20, 30), sharex=True)\n",
    "df_inst_tag_com = df_instagram[(df_instagram[\"is_comment\"] == True)].\\\n",
    "                       groupby(['tag', 'text_proc_sentiment_prob']).count().reset_index()\n",
    "df_inst_tag_post = df_instagram[(df_instagram[\"is_comment\"] == False)].\\\n",
    "                       groupby(['tag', 'text_proc_sentiment_prob']).count().reset_index()\n",
    "ax = sns.barplot(y=\"tag\", x=\"text\", hue=\"text_proc_sentiment_prob\", \n",
    "                 data=df_inst_tag_com[(df_inst_tag_com['text'] > MAX_REC) & (df_inst_tag_com['text'] < 10000)], ax=axes[0, 0])\n",
    "ax2 = sns.barplot(y=\"tag\", x=\"text\", hue=\"text_proc_sentiment_prob\",\n",
    "                  data=df_inst_tag_post[(df_inst_tag_post['text'] > MAX_REC) & (df_inst_tag_post['text'] < 10000)], ax=axes[0, 1])\n",
    "\n",
    "ax.set(xlabel='account or tag', ylabel='count of posts and comments')\n",
    "\n",
    "df_inst_tag_com = df_instagram[(df_instagram[\"is_comment\"] == True)].\\\n",
    "                       groupby(['tag', 'text_proc_sentiment']).count().reset_index()\n",
    "df_inst_tag_post = df_instagram[(df_instagram[\"is_comment\"] == False)].\\\n",
    "                       groupby(['tag', 'text_proc_sentiment']).count().reset_index()\n",
    "\n",
    "ax = sns.barplot(y=\"tag\", x=\"text\", hue=\"text_proc_sentiment\", \n",
    "                 data=df_inst_tag_com[(df_inst_tag_com['text'] > MAX_REC) & (df_inst_tag_com['text'] < 10000)],\n",
    "                 ax=axes[1, 0])\n",
    "ax2 = sns.barplot(y=\"tag\", x=\"text\", hue=\"text_proc_sentiment\",\n",
    "                  data=df_inst_tag_post[(df_inst_tag_post['text'] > MAX_REC) & (df_inst_tag_post['text'] < 10000)],\n",
    "                  ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst_tag_post = df_inst_tag_post[df_inst_tag_post['text'] > 200]\n",
    "yrs = np.unique(df_inst_tag_post['tag'].values)\n",
    "prs = []\n",
    "for y in yrs:\n",
    "    subdf = df_inst_tag_post[df_inst_tag_post['tag'] == y].groupby(['text_proc_sentiment']).sum()[['text']]\n",
    "    prs.append(subdf['text'].get(0, 0)/(subdf['text'].get(0, 0) + subdf['text'][4])*100)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('percent of negative texts')\n",
    "plt.plot(prs, yrs)            \n",
    "print(min(prs), max(prs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst_tag_com = df_inst_tag_com[df_inst_tag_com['text'] > 200]\n",
    "yrs = np.unique(df_inst_tag_com['tag'].values)\n",
    "prs = []\n",
    "for y in yrs:\n",
    "    subdf = df_inst_tag_com[df_inst_tag_com['tag'] == y].groupby(['text_proc_sentiment']).sum()[['text']]\n",
    "    prs.append(subdf['text'].get(0, 0)/(subdf['text'].get(0, 0) + subdf['text'][4])*100)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('percent of negative texts')\n",
    "plt.plot(prs, yrs)            \n",
    "print(min(prs), max(prs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_tags = np.unique(df_instagram_unique[df_instagram_unique[\"is_comment\"] == False]['tag'].values)\n",
    "tags_lists = list(map(lambda t: re.sub(\"[\\[\\]\\',]\", '', t).split(), df_instagram['tags'].values))\n",
    "flattened_tags = [val for sublist in tags_lists for val in sublist]\n",
    "unique, counts = np.unique(flattened_tags, return_counts=True)\n",
    "inst_counts = list(filter(lambda t: t[1] > 100 and t[0] not in original_tags, list(zip(unique, counts))))\n",
    "inst_counts = sorted(inst_counts, key=lambda t: -t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,30))\n",
    "df_instagram_tag_count = pd.DataFrame(data=inst_counts,columns=['tag', 'count'])\n",
    "ax = sns.barplot(y=\"tag\", x=\"count\", data=df_instagram_tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit['is_comment'] = df_reddit['is_comment'].progress_apply(lambda t: not t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique = df_reddit.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = len(df_reddit_unique[df_reddit_unique[\"is_comment\"] == True][\"id\"])\n",
    "publ = len(df_reddit_unique[df_reddit_unique[\"is_comment\"] == False][\"id\"])\n",
    "print(f'overall length: {len(df_reddit_unique[\"id\"])}, comments: {com}, publications: {publ}, publications with comments: {publ_with_com}')\n",
    "print(f'avg comments per publication: {com/publ}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_count(r):\n",
    "    if r['is_comment']:\n",
    "        return 0\n",
    "    return len(df_reddit_unique[df_reddit_unique['topic_id']==r['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique['comment_count'] = df_reddit_unique.progress_apply(lambda row: get_com_count(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_tag = df_reddit_unique.groupby(['subreddit_topic']).count().reset_index()\n",
    "df_reddit_tag = df_reddit_tag[df_reddit_tag['text'] > 1]\n",
    "df_reddit_tag['subreddit_topic'] = df_reddit_tag['subreddit_topic'].apply(lambda t: re.sub('.json', '', t))\n",
    "ax = sns.barplot(y=\"subreddit_topic\", x=\"text\", data=df_reddit_tag)\n",
    "ax.set(ylabel='Subreddit', xlabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(r):\n",
    "    try:\n",
    "        return float(r)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "df_reddit_unique[\"score\"] = df_reddit_unique[\"score\"].apply(lambda t: conv(t))\n",
    "df_reddit_unique[\"com_count\"] = df_reddit_unique[\"com_count\"].apply(lambda t: conv(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.distplot(df_reddit_unique[\"score\"], \n",
    "             kde=False, rug=True, bins=10)\n",
    "ax.set(xlabel='score', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.distplot(df_reddit_unique[\"com_count\"], \n",
    "             kde=False, rug=True, bins=10)\n",
    "ax.set(xlabel='comments count', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique[df_reddit_unique['subreddit_topic'] == 'intently gay nb btw love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram.sort_values(['comments_count'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tm_by_methods(df_instagram, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after prep changes\n",
    "tm_by_methods(df_instagram, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#after prep changes\n",
    "tm_by_methods(df_instagram, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_graph_tokens(df_instagram[df_instagram['is_comment']]['text_proc_tm'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_graph_tokens(df_instagram[df_instagram['is_comment'] == False]['text_proc_tm'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_graph_tokens(df_twitter['text_proc_tm'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mins in range(5, 40, 5):\n",
    "    for maxs in range(5, 10):\n",
    "        for topics in range(0, 20):\n",
    "            print(mins, maxs,topics)\n",
    "            compare_models(df_instagram_unique, mins, 0.1*maxs, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_models(df_twitter_unique, 20, 0.8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_models(df_reddit_unique, 20, 0.8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(df, min_corp, max_corp, num_topics):\n",
    "    lda_model, lda_corpus, lda_id2word, lda_texts_proc = get_tm_model_simple(\n",
    "        df['text_proc_tm'], 'lda', min_corp, max_corp, num_topics)\n",
    "    ldatopics = lda_model.show_topics(formatted=False)\n",
    "    lsi_model, lsi_corpus, lsi_id2word, lsi_texts_proc = get_tm_model_simple(\n",
    "        df['text_proc_tm'], 'lsi', min_corp, max_corp, num_topics)\n",
    "    lsitopics = lsi_model.show_topics(formatted=False)\n",
    "    hdp_model, hdp_corpus, hdp_id2word, hdp_texts_proc = get_tm_model_simple(\n",
    "        df['text_proc_tm'], 'hdp', min_corp, max_corp, num_topics)\n",
    "    hdptopics = hdp_model.show_topics(formatted=False)\n",
    "\n",
    "    ldatopics = [[word for word, prob in topic]\n",
    "                 for topicid, topic in ldatopics]\n",
    "    lsitopics = [[word for word, prob in topic]\n",
    "                 for topicid, topic in lsitopics]\n",
    "    hdptopics = [[word for word, prob in topic]\n",
    "                 for topicid, topic in hdptopics]\n",
    "\n",
    "    lsi_coherence = CoherenceModel(\n",
    "        topics=lsitopics[:10], texts=lda_texts_proc, dictionary=lda_id2word, window_size=10, processes=1).get_coherence()\n",
    "    hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=lsi_texts_proc,\n",
    "                                   dictionary=lsi_id2word, window_size=10, processes=1).get_coherence()\n",
    "    lda_coherence = CoherenceModel(topics=ldatopics, texts=hdp_texts_proc,\n",
    "                                   dictionary=hdp_id2word, window_size=10, processes=1).get_coherence()\n",
    "\n",
    "    evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence, ],\n",
    "                       ['LSI', 'HDP', 'LDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lda_corpus, lda_id2word, lda_texts_proc = get_tm_model_simple(df_instagram['text_proc_tm'], 'lda')\n",
    "ldatopics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "lsi_model, lsi_corpus, lsi_id2word, lsi_texts_proc = get_tm_model_simple(df_instagram['text_proc_tm'], 'lsi')\n",
    "lsitopics = lsi_model.show_topics(formatted=False)\n",
    "\n",
    "hdp_model, hdp_corpus, hdp_id2word, hdp_texts_proc = get_tm_model_simple(df_instagram['text_proc_tm'], 'hdp')\n",
    "hdptopics = hdp_model.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldatopics]\n",
    "\n",
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsitopics]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdptopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=lda_texts_proc, dictionary=lda_id2word, window_size=10, processes=1).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=lsi_texts_proc, dictionary=lsi_id2word, window_size=10, processes=1).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=hdp_texts_proc, dictionary=hdp_id2word, window_size=10, processes=1).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence,],\n",
    "                   ['LSI', 'HDP', 'LDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_posts = df_instagram.loc[df_instagram['is_comment'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_instagram_group(df, is_comment, likes_min, likes_max, comments_min, comments_max):\n",
    "    sub_df_instagram = df_instagram.loc[df_instagram['is_comment'] == is_comment]\n",
    "    sub_df_instagram = sub_df_instagram[(sub_df_instagram['likes'] >= likes_min) & (sub_df_instagram['likes'] <= likes_max)]\n",
    "    sub_df_instagram = sub_df_instagram[(sub_df_instagram['comments_count'] >= comments_min) & (sub_df_instagram['comments_count'] <= comments_max)]\n",
    "    \n",
    "    sub_df_instagram['likes'].hist()\n",
    "    print(len(sub_df_instagram))\n",
    "    print('OVERALL')\n",
    "    cm = construct_topics(sub_df_instagram['text_proc_tm'], 'lda', 2, 10, 5, True)\n",
    "    cm = construct_topics(sub_df_instagram['text_proc_tm'], 'lda', 10, 10, 5, True)\n",
    "    print('=====')\n",
    "    cm = construct_topics(sub_df_instagram['text_proc_tm'], 'lsi', 2, 10, 5, True)\n",
    "    cm = construct_topics(sub_df_instagram['text_proc_tm'], 'lsi', 10, 10, 5, True)\n",
    "    print('=====')\n",
    "    cm = construct_topics(sub_df_instagram['text_proc_tm'], 'hdp', 2, 10, 5, True)\n",
    "    cm = construct_topics(sub_df_instagram['text_proc_tm'], 'hdp', 10, 10, 5, True)\n",
    "    print('BY POST')\n",
    "    process_nmf(sub_df_instagram['text_proc_tm'], 2, 10)\n",
    "    process_nmf(sub_df_instagram['text_proc_tm'], 10,  10)\n",
    "            \n",
    "    group_by_data(sub_df_instagram, 'parent_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "construct_topics(df_instagram_unique['text_proc_tm'], 'lda', 2, 10, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_graph(df_instagram['text_proc_tm'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_graph(df_instagram['text_proc_tm'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_graph(df_instagram['text_proc_tm'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_graph(df_instagram['text_proc_tm'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter_unique = df_twitter.drop_duplicates(subset=[\"id\"])\n",
    "print(f'count of tweets: {len(df_twitter_unique)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_twitter['loc_country_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = np.unique(df_twitter['loc_country'].values)\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_grouped_location = df_twitter_unique[df_twitter_unique['loc_country'] != ''].groupby(['loc_country']).count().reset_index()\n",
    "print(len(twitter_grouped_location[twitter_grouped_location['text'] > 20]))\n",
    "twitter_grouped_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( df_twitter_unique[df_twitter_unique['loc_country'] != '']) / len( df_twitter_unique)* 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,20))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.barplot(y=\"loc_country\", x=\"text\", data=twitter_grouped_location[twitter_grouped_location['text'] > 5])\n",
    "ax.set(ylabel='country code', xlabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.distplot(df_twitter_unique[\"favorite_count\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax.set(xlabel='number of retweet', ylabel='count of tweets')\n",
    "ax1 = sns.distplot(df_twitter_unique[df_twitter_unique[\"favorite_count\"] < 1000][\"favorite_count\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax1.set(xlabel='number of favorite marks', ylabel='count of tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.distplot(df_twitter_unique[\"retweet_count\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax.set(xlabel='number of retweet', ylabel='count of tweets')\n",
    "ax1 = sns.distplot(df_twitter_unique[df_twitter_unique[\"retweet_count\"] < 100000][\"retweet_count\"], \n",
    "             kde=False, rug=True, bins=5, label=\"sdfs\")\n",
    "ax1.set(xlabel='number of retweet', ylabel='count of tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_lists = list(map(lambda t: re.sub(\"[\\[\\]\\',]\", '', t).split(), df_twitter_unique['tags'].values))\n",
    "flattened_tags = [val for sublist in tags_lists for val in sublist]\n",
    "unique, counts = np.unique(flattened_tags, return_counts=True)\n",
    "inst_counts = list(filter(lambda t: t[1] > 200 and t[0] not in original_tags, list(zip(unique, counts))))\n",
    "inst_counts = sorted(inst_counts, key=lambda t: -t[1])\n",
    "plt.figure(figsize=(8,12))\n",
    "df_instagram_tag_count = pd.DataFrame(data=inst_counts,columns=['tag', 'count'])\n",
    "ax = sns.barplot(y=\"tag\", x=\"count\", data=df_instagram_tag_count)\n",
    "ax1.set(xlabel='count of tweets', ylabel='hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['text_proc_sentiment_prob'] = \\\n",
    "    df_twitter['text_proc_sentiment_prob_pos'].\\\n",
    "        progress_apply(lambda t: 4 if t >= 0.675 else 0 if t <= 0.33 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_lists = list(map(lambda t: re.sub(\"[\\[\\]\\',]\", '', t).split(), df_twitter_unique['tags'].values))\n",
    "df_twitter_unique['tags_ar'] = df_twitter_unique['tags'].apply(lambda t: re.sub(\"[\\[\\]\\',]\", '', t).\\\n",
    "                                                               split()).apply(np.array)\n",
    "df_twitter_unique_with_tags = df_twitter_unique.explode('tags_ar')\n",
    "df_twitter_unique_with_tags = df_twitter_unique_with_tags[df_twitter_unique_with_tags['tags_ar'].isnull() == False]\n",
    "df_twitter_unique_with_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REC = 100\n",
    "f, axes = plt.subplots(1, 2, figsize=(35, 30), sharex=True)\n",
    "df_tw_tag = df_twitter_unique_with_tags.\\\n",
    "                       groupby(['tags_ar', 'text_proc_sentiment_prob']).count().reset_index()\n",
    "ax = sns.barplot(y=\"tags_ar\", x=\"text\", hue=\"text_proc_sentiment_prob\", \n",
    "                 data=df_tw_tag[df_tw_tag['text'] > MAX_REC], ax=axes[0])\n",
    "ax.set(xlabel='account or tag', ylabel='count of posts and comments')\n",
    "\n",
    "df_tw_tag = df_twitter_unique_with_tags.\\\n",
    "                       groupby(['tags_ar', 'text_proc_sentiment']).count().reset_index()\n",
    "ax = sns.barplot(y=\"tags_ar\", x=\"text\", hue=\"text_proc_sentiment\", \n",
    "                 data=df_tw_tag[df_tw_tag['text'] > MAX_REC], ax=axes[1])\n",
    "ax.set(xlabel='account or tag', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_unique['date_info'] = df_twitter_unique['date'].dt.month.map(str) + \"-\" + df_twitter_unique['date'].dt.year.map(str)\n",
    "df_twitter_unique['year'] = df_twitter_unique['date'].dt.year.map(str)\n",
    "df_twitter_unique_grouped = df_twitter_unique.groupby(['date_info']).count().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.xlabel(\"count of comments per post\")\n",
    "plt.ylabel(\"count of posts\")\n",
    "ax = sns.barplot(x=\"date_info\", y=\"text\", data=df_twitter_unique_grouped)\n",
    "ax.set(xlabel='year of publications', ylabel='count of posts and comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_twitter_group(df, param, param_min, param_max):\n",
    "    sub_df = df[(df[param] >= param_min) & (df[param] <= param_max)]\n",
    "    \n",
    "    print(len(sub_df))\n",
    "    print('OVERALL')\n",
    "    cm = construct_topics(sub_df['text_proc_tm'], 'lda', 2, True)\n",
    "    cm = construct_topics(sub_df['text_proc_tm'], 'lda', 10, True)\n",
    "    process_nmf(sub_df['text_proc_tm'], 2)\n",
    "    process_nmf(sub_df['text_proc_tm'], 10)\n",
    "            \n",
    "    print(f'BY {param}')\n",
    "    group_by_data(sub_df, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_twitter_group(df_twitter, 'retweet_count', 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_twitter_group(df_twitter, 'retweet_count', 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_twitter_group(df_twitter, 'retweet_count', 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_twitter_group(df_twitter, 'retweet_count', 1000, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.sort_values(['retweet'], ascending=[False])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter[df_twitter['retweet'] > 10000]['retweet'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    print(f'number of topics {i}')\n",
    "    process_nmf(df_twitter['text_proc_tm'], i)\n",
    "    print('-------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    print(f'number of topics {i}')\n",
    "    process_nmf(df_reddit['text_proc'], i)\n",
    "    print('-------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    print(f'number of topics {i}')\n",
    "    process_nmf(df_reddit['text_proc'], i)\n",
    "    print('-------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    print(f'number of topics {i}')\n",
    "    process_nmf(df_twitter['text_proc'], i)\n",
    "    print('-------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    print(f'number of topics {i}')\n",
    "    process_nmf(df_twitter['text_proc'], i)\n",
    "    print('-------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(data=cms,columns=['parameter', 'method', 'num_topics', 'cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in df_cm.values:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data(df_reddit, 'subreddit_topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = construct_topics(df_instagram['text_proc'], 'lda', 15, True)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis = construct_topics(df_twitter['text_proc'], 'lda', 5, True)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = construct_topics(df_reddit['text_proc'], 'lda', 5, True)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis = construct_topics(df_twitter['text_proc'], 'lsi', 10, True)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = construct_topics(df_twitter['text_proc'], 'hdp', 10, True)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunksize 400 and min 5 max 0.75\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_instagram_unique['text_proc_tm'], 'lsi', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#chunksize 400 and min 5 max 0.75\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_instagram_unique['text_proc_tm'], 'lsi', 20, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, corpus, _dict = construct_topics(df_instagram_unique['text_proc_tm'], 'lsi', 20, 0.8, 11, 12, 5, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_for_topic(model, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, corpus, _dict = construct_topics(df_instagram_unique['text_proc_tm'], 'lsi', 20, 0.8, 7, 12, 5, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_for_topic(model, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no chunksize and min 5 max 0.75\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_instagram_unique['text_proc_tm'], 'hdp', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunksize 400, min 10\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_instagram_unique['text_proc_tm'], 'hdp', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no chunksize and min 5 max 0.75\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_instagram_unique['text_proc_tm'], 'lsi', i, 12, 5, True, 20, 0.65)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunksize 400, min 10 max 0.75\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_instagram_unique['text_proc_tm'], 'hdp', i, 12, 5, True, 20, 0.65)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_unique['text_proc_tm'], 'lsi', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max 0.75, min 5, no chunksize\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_unique['text_proc_tm'], 'hdp', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_reddit_unique['text_proc_tm'], 'lsi', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_reddit_unique['text_proc_tm'], 'hdp', i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_unique_with_likes = df_instagram_unique[df_instagram_unique.likes > 0]\n",
    "df_instagram_unique_without_likes = df_instagram_unique[(df_instagram_unique.likes == 0)\\\n",
    "                                                        & (df_instagram_unique.is_comment == False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_instagram_unique_with_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_instagram_unique_without_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst_unpop = df_instagram_unique_with_likes[df_instagram_unique_with_likes.likes <= df_instagram_unique_with_likes.likes.quantile(0.1)]\n",
    "print(f'instagram percentile 0.1 likes {len(df_inst_unpop)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_inst_unpop['text_proc_tm'], 'lsi', 20, 0.65, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Topic Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, df = construct_topics(df_inst_unpop['text_proc_tm'], 'lsi', 20, 0.65, 6, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consruct_topics_with_graph(df_sa, model, mins, maxs, num_topics, c, k, b, param):\n",
    "    cm, df, df1 = construct_topics_df(df_sa, model, mins, maxs, num_topics, c, k, b)\n",
    "    df = df.reset_index()\n",
    "    df1 = df1.reset_index()\n",
    "    mrjd = pd.concat([df, df1], axis=1)\n",
    "    mrjd = mrjd[mrjd[param].isnull() == False]\n",
    "    plt.figure()\n",
    "    ax = sns.countplot(y=\"keywords\", hue=param, data=mrjd)\n",
    "    ax.set_xlabel('Count of texts')\n",
    "    ax.set_ylabel('Topic Keywords')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_inst_unpop, 'lsi', 20, 0.65, 6, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_inst_unpop, 'lsi', 20, 0.65, 6, 12, 5, True, \"text_proc_sentiment_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#chunksize equal to all\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_inst_unpop['text_proc_tm'], 'lsi', 20, 0.65, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst_pop = df_instagram_unique_with_likes[df_instagram_unique_with_likes.likes >= df_instagram_unique_with_likes.likes.quantile(0.9)]\n",
    "print(f'instagram percentile 0.9 likes {len(df_inst_pop)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_inst_pop, 'lsi', 20, 0.65, 7, 3, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_inst_pop, 'lsi', 20, 0.65, 7, 3, 1, True, \"text_proc_sentiment_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_inst_pop['text_proc_tm'], 'lsi', 20, 0.65, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunksize equal to all\n",
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_inst_pop['text_proc_tm'], 'lsi', 20, 0.65, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_unique_1 = df_twitter_unique[df_twitter_unique['is_quote_status'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_unique_with_likes = df_twitter_unique_1[df_twitter_unique_1['retweet_count'] > 0]\n",
    "print(f'twitter records with retweets {len(df_twitter_unique_with_likes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_pop = df_twitter_unique_with_likes[df_twitter_unique_with_likes.retweet_count >= \\\n",
    "                                             df_twitter_unique_with_likes.retweet_count.quantile(0.85)]\n",
    "print(f'twitter percentile 0.9 likes {len(df_twitter_pop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_unpop = df_twitter_unique_with_likes[df_twitter_unique_with_likes.retweet_count <= \\\n",
    "                                             df_twitter_unique_with_likes.retweet_count.quantile(0.1)]\n",
    "print(f'twitter percentile 0.1 likes {len(df_twitter_unpop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_pop['text_proc_tm'], 'lsi', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " construct_topics(df_twitter_pop['text_proc_tm'], 'lsi', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_twitter_pop, 'lsi', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " construct_topics(df_twitter_pop['text_proc_tm'], 'lda', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_twitter_pop, 'lda', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_twitter_pop, 'lda', 5, 0.8, 3, 12, 5, True, \"text_proc_sentiment_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " consruct_topics_with_graph(df_twitter_pop, 'lda', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " consruct_topics_with_graph(df_twitter_pop, 'lsi', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_pop['text_proc_tm'], 'lda', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lsi', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_topics(df_twitter_pop['text_proc_tm'], 'lda', 5, 0.8, 1, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_topics(df_twitter_pop['text_proc_tm'], 'lda', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_topics(df_twitter_unpop['text_proc_tm'], 'lda', 5, 0.8, 3, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_twitter_unpop, 'lda', 5, 0.8, 18, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lsi', 5, 0.8, 2, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lda', 5, 0.8, 10, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lda', 5, 0.8, 2, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lsi', 10, 0.65, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lda', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_twitter_unpop['text_proc_tm'], 'lda', 10, 0.65, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " construct_topics(df_twitter_unpop['text_proc_tm'], 'lda', 10, 0.65, 18, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " consruct_topics_with_graph(df_twitter_unpop, 'lda', 10, 0.65, 18, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_twitter_unpop, 'lda', 10, 0.65, 18, 12, 5, True, \"text_proc_sentiment_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_score(r):\n",
    "    print(r)\n",
    "    try:\n",
    "        r = float(r)\n",
    "        return r > 0\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique_with_likes = df_reddit_unique[df_reddit_unique['score'].str.isnumeric() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique_with_likes['score'] = df_reddit_unique_with_likes['score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique_with_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unique_with_likes = df_reddit_unique_with_likes[(df_reddit_unique_with_likes['score'] != 0)\\\n",
    "                                                         & (df_reddit_unique_with_likes['parent_id'] == '')]\n",
    "print(f'twitter records with retweets {len(df_reddit_unique_with_likes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_unpop = df_reddit_unique_with_likes[df_reddit_unique_with_likes.score \\\n",
    "                                            <= df_reddit_unique_with_likes.score.quantile(0.1)]\n",
    "print(f'instagram percentile 0.1 likes {len(df_reddit_unpop)}')\n",
    "\n",
    "\n",
    "df_reddit_pop = df_reddit_unique_with_likes[df_reddit_unique_with_likes.score \\\n",
    "                                            >= df_reddit_unique_with_likes.score.quantile(0.9)]\n",
    "print(f'instagram percentile 0.1 likes {len(df_reddit_pop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_reddit_unpop['text_proc_tm'], 'lda', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_reddit_unpop['text_proc_tm'], 'lsi', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "construct_topics(df_reddit_unpop['text_proc_tm'], 'lda', 5, 0.8, 6, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_reddit_unpop, 'lda', 5, 0.8, 6, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_reddit_unpop, 'lda', 5, 0.8, 6, 12, 5, True, \"text_proc_sentiment_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_reddit_pop['text_proc_tm'], 'lda', 5, 0.9, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    cm = construct_topics(df_reddit_pop['text_proc_tm'], 'lsi', 5, 0.8, i, 12, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_reddit_pop, 'lda', 5, 0.8, 1, 12, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consruct_topics_with_graph(df_reddit_pop, 'lda', 5, 0.8, 1, 12, 5, True, \"text_proc_sentiment_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_lists = list(map(lambda t: re.sub(\"[\\[\\]\\',]\", '', t).split(), df_twitter_unique['tags'].values))\n",
    "df_instagram['tags_ar'] = df_instagram['tags'].apply(lambda t: re.sub(\"[\\[\\]\\',]\", '', t).\\\n",
    "                                                               split()).apply(np.array)\n",
    "df_instagram_with_tags = df_instagram.explode('tags_ar')\n",
    "df_instagram_with_tags = df_instagram_with_tags[df_instagram_with_tags['tags_ar'].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_flattened_tags = list(np.unique(df_twitter_unique_with_tags['tags_ar'].values))\n",
    "instagram_flattened_tags = list(np.unique(df_instagram_with_tags['tags_ar'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(instagram_flattened_tags)), len(set(twitter_flattened_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_intersection = list(set(inst_flattened_tags).intersection(set(twitter_flattened_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_with_tags_int = df_instagram_with_tags[df_instagram_with_tags['tags_ar'].\\\n",
    "                                                    map(lambda t: t in tags_intersection)]\\\n",
    "                                                    [['text_proc_tm', 'text_proc_sentiment',\n",
    "                                                      'text_proc_sentiment_prob', 'tags_ar']]\n",
    "df_instagram_with_tags_int['source'] = 'instagram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_with_tags_int = df_twitter_unique_with_tags[df_twitter_unique_with_tags['tags_ar'].\\\n",
    "                                                    map(lambda t: t in tags_intersection)]\\\n",
    "                                                    [['text_proc_tm', 'text_proc_sentiment',\n",
    "                                                      'text_proc_sentiment_prob', 'tags_ar']]\n",
    "df_twitter_with_tags_int['source'] = 'twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instagram_with_tags_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_with_tags_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = df_twitter_with_tags_int.append(df_instagram_with_tags_int, ignore_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOPICS = 20\n",
    "ars = []\n",
    "for tag in tags_intersection:\n",
    "    subdf = joined.loc[joined['tags_ar'] == tag]\n",
    "    print(f'tag: {tag}, size: {len(subdf)}')\n",
    "    if len(subdf) > 100:\n",
    "        print('start processing')\n",
    "        cms = []\n",
    "        max_cm = 0\n",
    "        max_ind = -1\n",
    "        try:\n",
    "            for i in range(1, MAX_TOPICS, 2):\n",
    "                cm = consruct_topics_with_graph(subdf, 'lda', 10, 0.65, i, 20, 5, True, \"text_proc_sentiment\")\n",
    "                if cm > max_cm:\n",
    "                    max_cm = cm\n",
    "                    max_ind = i\n",
    "                cms.append(cm)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        print(\"DONE\")\n",
    "        x = range(1, MAX_TOPICS)\n",
    "        print(cms, len(x), len(cms))\n",
    "        if len(x) == len(cms):\n",
    "            print(f'TAG: {tag}, MAX CM: {max_cm}')\n",
    "            ars.append([tag, max_ind, max_cm])\n",
    "            plt.figure()\n",
    "            plt.plot(x, cms)\n",
    "            plt.xlabel(\"num_topics\")\n",
    "            plt.ylabel(\"Coherence score\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'BlackLivesMatter']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lsi', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'Diversity']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'Diversity']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lsi', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'lockdown']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 20, 0.8, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == 'BlackLivesMatter']\n",
    "consruct_topics_with_graph(subdf, 'lsi', 10, 0.65, 2, 20, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'Women']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == 'Women']\n",
    "consruct_topics_with_graph(subdf, 'lsi', 10, 0.8, 3, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'whyididntreport']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 5, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == 'whyididntreport']\n",
    "consruct_topics_with_graph(subdf, 'lda', 5, 0.65, 8, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'metoo']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'art']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == 'art']\n",
    "consruct_topics_with_graph(subdf, 'lda', 10, 0.65, 3, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'mentalhealth']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.8, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'motivation']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'Equality']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == 'Equality']\n",
    "consruct_topics_with_graph(subdf, 'lda', 10, 0.65, 13, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'beautiful']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lsi', 10, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == 'beautiful']\n",
    "consruct_topics_with_graph(subdf, 'lsi', 5, 0.65, 3, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'womeninbusiness']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 5, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = joined.loc[joined['tags_ar'] == '5']\n",
    "consruct_topics_with_graph(subdf, 'lda', 5, 0.65, 5, 12, 5, True, \"text_proc_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "subdf = joined.loc[joined['tags_ar'] == 'covid19']\n",
    "for i in range(1, 20):\n",
    "    cm, df = construct_topics(subdf['text_proc_tm'], 'lda', 5, 0.65, i, 20, 5, True)\n",
    "    cms.append(cm)\n",
    "    \n",
    "print(cms)\n",
    "plt.figure()\n",
    "x = range(1, 20)\n",
    "plt.plot(x, cms)\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "200.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
